nohup: 忽略输入
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2023-08-14 16:21:03,865] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,872] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,896] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,911] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,916] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,931] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:03,933] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-14 16:21:04,179] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,179] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,180] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,180] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,195] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,195] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,209] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,209] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,229] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,229] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,229] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-08-14 16:21:04,230] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,230] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,232] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,232] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-08-14 16:21:04,236] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-08-14 16:21:04,236] [INFO] [comm.py:616:init_distributed] cdb=None
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:710] 2023-08-14 16:21:04,446 >> loading configuration file /data3/litian/Redemption/LLama-2/chat/7B_HF/config.json
[INFO|configuration_utils.py:768] 2023-08-14 16:21:04,447 >> Model config LlamaConfig {
  "_name_or_path": "/data3/litian/Redemption/LLama-2/chat/7B_HF",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1837] 2023-08-14 16:21:04,450 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1837] 2023-08-14 16:21:04,450 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1837] 2023-08-14 16:21:04,450 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1837] 2023-08-14 16:21:04,450 >> loading file tokenizer_config.json
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:04,454 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:295] 2023-08-14 16:21:04,457 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:04 - INFO - __main__ - training files: /data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json
08/14/2023 16:21:04 - WARNING - root - building dataset...
08/14/2023 16:21:04 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:04,706 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:04,810 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:04,816 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:04 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:04,887 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:05 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:05,020 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:05 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True
[WARNING|logging.py:295] 2023-08-14 16:21:05,398 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
08/14/2023 16:21:08 - INFO - __main__ - Num train_samples  500000
08/14/2023 16:21:08 - INFO - __main__ - training example:
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Human:你好，你能帮我翻译一段英文吗？
Assistant: 当然可以，您可以把需要翻译的英文句子发给我吗？
Human:这是一段关于机器翻译的论文，有些专业术语我不是很懂。
Assistant:

### Response:   没问题，我会尽力帮您解决翻译中遇到的问题。请您放心地将论文的句子发送给我，让我来为您翻译。</s>
08/14/2023 16:21:08 - INFO - __main__ - training files: /data3/litian/Redemption/litian_data/validation_100.json
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/Belle_alpaca_json_500000/firstTry-Bella+alpaca-500000.json has been loaded from disk
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - WARNING - root - building dataset...
08/14/2023 16:21:08 - INFO - __main__ - Num eval_samples  100
08/14/2023 16:21:08 - INFO - __main__ - eval example:
08/14/2023 16:21:08 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
为给定的产品或品牌生成一句简短的广告口号。
品牌或产品：Nike（耐克）


### Response:  Just Do It.</s>
[INFO|modeling_utils.py:2600] 2023-08-14 16:21:08,750 >> loading weights file /data3/litian/Redemption/LLama-2/chat/7B_HF/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1172] 2023-08-14 16:21:08,753 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|modeling_utils.py:2694] 2023-08-14 16:21:08,754 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
[INFO|configuration_utils.py:599] 2023-08-14 16:21:08,758 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
08/14/2023 16:21:08 - INFO - __name__ - training datasets-/data3/litian/Redemption/litian_data/validation_100.json has been loaded from disk
[2023-08-14 16:21:11,149] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.96s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.08s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:52<00:52, 52.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:07<00:00, 30.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:07<00:00, 34.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 30.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 30.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 30.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 30.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 31.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 30.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 31.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.17s/it]
[INFO|modeling_utils.py:3329] 2023-08-14 16:22:19,526 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3337] 2023-08-14 16:22:19,528 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data3/litian/Redemption/LLama-2/chat/7B_HF.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:559] 2023-08-14 16:22:19,532 >> loading configuration file /data3/litian/Redemption/LLama-2/chat/7B_HF/generation_config.json
[INFO|configuration_utils.py:599] 2023-08-14 16:22:19,532 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

08/14/2023 16:22:19 - INFO - __main__ - len(tokenizer):49954
08/14/2023 16:22:19 - INFO - __main__ - resize the embedding size by the size of the tokenizer
08/14/2023 16:22:23 - INFO - __main__ - Init new peft model
08/14/2023 16:22:23 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']
08/14/2023 16:22:23 - INFO - __main__ - lora_rank: 8
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/home/litian/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.8544144630432129 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.7100765705108643 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.8633644580841064 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.6670796871185303 seconds
08/14/2023 16:23:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 4
08/14/2023 16:23:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/14/2023 16:23:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 5
08/14/2023 16:23:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 7
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
08/14/2023 16:23:53 - INFO - __main__ - model.modules_to_save: {'lm_head', 'embed_tokens'}
[INFO|trainer.py:565] 2023-08-14 16:23:53,285 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|deepspeed.py:291] 2023-08-14 16:23:53,453 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
trainable params: 838,434,816 || all params: 7,314,706,432 || trainable%: 11.462316687544133
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/project/anaconda3_test/envs/QLoRA/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/home/litian/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.9988346099853516 seconds
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000200, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2023-08-14 16:23:56,291] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Using /mnt/home/litian/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
08/14/2023 16:23:56 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/home/litian/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
08/14/2023 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:2 (world_size=8, worker_count=5, timeout=0:30:00)
ninja: no work to do.
08/14/2023 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:2 (world_size=8, worker_count=5, timeout=0:30:00)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 1.0342090129852295 seconds
Time to load cpu_adam op: 0.8030593395233154 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.8278224468231201 seconds
08/14/2023 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:2 (world_size=8, worker_count=5, timeout=0:30:00)
08/14/2023 16:23:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 7, key: store_based_barrier_key:2 (world_size=8, worker_count=5, timeout=0:30:00)
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 6
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/14/2023 16:23:57 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
[2023-08-14 16:23:58,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-14 16:23:58,579] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-14 16:23:58,579] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-14 16:23:58,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-14 16:23:58,620] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-14 16:23:58,620] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-08-14 16:23:58,620] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2023-08-14 16:23:58,809] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-08-14 16:23:58,810] [INFO] [utils.py:786:see_memory_usage] MA 1.59 GB         Max_MA 1.63 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:23:58,812] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 57.8 GB, percent = 18.4%
[2023-08-14 16:23:58,822] [INFO] [stage3.py:117:__init__] Reduce bucket size 16777216
[2023-08-14 16:23:58,822] [INFO] [stage3.py:118:__init__] Prefetch bucket size 15099494
[2023-08-14 16:23:58,986] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-14 16:23:58,987] [INFO] [utils.py:786:see_memory_usage] MA 1.59 GB         Max_MA 1.59 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:23:58,989] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.44 GB, percent = 18.6%
Parameter Offload: Total persistent parameters: 11800576 in 417 params
[2023-08-14 16:23:59,529] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-14 16:23:59,530] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 1.59 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:23:59,532] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.92 GB, percent = 18.7%
[2023-08-14 16:23:59,706] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-08-14 16:23:59,707] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:23:59,709] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.94 GB, percent = 18.7%
[2023-08-14 16:24:00,127] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1
[2023-08-14 16:24:00,128] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:00,132] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.45 GB, percent = 20.2%
[2023-08-14 16:24:00,302] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-08-14 16:24:00,302] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:00,304] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.45 GB, percent = 20.5%
[2023-08-14 16:24:00,667] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-08-14 16:24:00,668] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:00,670] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.22 GB, percent = 20.4%
[2023-08-14 16:24:00,860] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-08-14 16:24:00,861] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:00,864] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 67.7 GB, percent = 21.5%
[2023-08-14 16:24:01,963] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-08-14 16:24:01,964] [INFO] [utils.py:786:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:01,967] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 76.07 GB, percent = 24.2%
[2023-08-14 16:24:02,166] [INFO] [stage3.py:424:_setup_for_real_optimizer] optimizer state initialized
[2023-08-14 16:24:02,988] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-08-14 16:24:02,989] [INFO] [utils.py:786:see_memory_usage] MA 0.06 GB         Max_MA 0.82 GB         CA 1.64 GB         Max_CA 2 GB 
[2023-08-14 16:24:02,992] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 76.49 GB, percent = 24.3%
[2023-08-14 16:24:02,992] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-08-14 16:24:02,992] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-14 16:24:02,992] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-14 16:24:02,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-08-14 16:24:02,999] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-14 16:24:02,999] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-14 16:24:03,001] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-14 16:24:03,001] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-14 16:24:03,001] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f33a09fd4f0>
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-14 16:24:03,002] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1e-10}
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-14 16:24:03,004] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-14 16:24:03,005] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   train_batch_size ............. 128
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  16
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   world_size ................... 8
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-14 16:24:03,007] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3
[2023-08-14 16:24:03,008] [INFO] [config.py:950:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 100, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1e-10
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 16, 
    "wall_clock_breakdown": false, 
    "bf16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[INFO|trainer.py:1686] 2023-08-14 16:24:03,010 >> ***** Running training *****
[INFO|trainer.py:1687] 2023-08-14 16:24:03,010 >>   Num examples = 500,000
[INFO|trainer.py:1688] 2023-08-14 16:24:03,010 >>   Num Epochs = 1
[INFO|trainer.py:1689] 2023-08-14 16:24:03,010 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1692] 2023-08-14 16:24:03,010 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1693] 2023-08-14 16:24:03,010 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1694] 2023-08-14 16:24:03,010 >>   Total optimization steps = 100
[INFO|trainer.py:1695] 2023-08-14 16:24:03,017 >>   Number of trainable parameters = 838,434,816
  0%|          | 0/100 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,269 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[WARNING|logging.py:295] 2023-08-14 16:24:03,270 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-08-14 16:24:11,466] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  1%|          | 1/100 [00:08<13:56,  8.45s/it]                                               {'loss': 7.7005, 'learning_rate': 0.0, 'epoch': 0.0}
  1%|          | 1/100 [00:08<13:56,  8.45s/it][2023-08-14 16:24:19,403] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  2%|▏         | 2/100 [00:16<13:18,  8.15s/it][2023-08-14 16:24:27,088] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  3%|▎         | 3/100 [00:24<12:49,  7.94s/it][2023-08-14 16:24:34,759] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
  4%|▍         | 4/100 [00:31<12:31,  7.83s/it][2023-08-14 16:24:42,409] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
  5%|▌         | 5/100 [00:39<12:17,  7.77s/it][2023-08-14 16:24:50,125] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
  6%|▌         | 6/100 [00:47<12:08,  7.75s/it][2023-08-14 16:24:57,837] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
  7%|▋         | 7/100 [00:54<11:59,  7.74s/it]  8%|▊         | 8/100 [01:03<12:09,  7.93s/it][2023-08-14 16:25:13,770] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
  9%|▉         | 9/100 [01:10<11:52,  7.82s/it] 10%|█         | 10/100 [01:18<11:55,  7.95s/it]                                                {'loss': 8.5283, 'learning_rate': 0.00019994965423831854, 'epoch': 0.0}
 10%|█         | 10/100 [01:18<11:55,  7.95s/it][INFO|trainer.py:3081] 2023-08-14 16:25:22,030 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:25:22,031 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:25:22,031 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 8.131863594055176, 'eval_runtime': 2.0926, 'eval_samples_per_second': 47.788, 'eval_steps_per_second': 0.478, 'epoch': 0.0}
 10%|█         | 10/100 [01:21<11:55,  7.95s/it]
100%|██████████| 1/1 [00:00<00:00,  5.80it/s][A
                                             [ASave Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:25:38,578 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:25:38,586 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10
 11%|█         | 11/100 [01:48<21:28, 14.48s/it][2023-08-14 16:25:59,017] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
 12%|█▏        | 12/100 [01:56<18:13, 12.43s/it][2023-08-14 16:26:06,646] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
 13%|█▎        | 13/100 [02:03<15:54, 10.97s/it][2023-08-14 16:26:14,309] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64
 14%|█▍        | 14/100 [02:11<14:17,  9.97s/it][2023-08-14 16:26:22,059] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32
 15%|█▌        | 15/100 [02:19<13:10,  9.30s/it] 16%|█▌        | 16/100 [02:27<12:31,  8.95s/it] 17%|█▋        | 17/100 [02:35<12:01,  8.69s/it] 18%|█▊        | 18/100 [02:43<11:36,  8.50s/it] 19%|█▉        | 19/100 [02:51<11:17,  8.36s/it] 20%|██        | 20/100 [02:59<11:01,  8.26s/it]                                                {'loss': 7.3531, 'learning_rate': 0.00019754297868854073, 'epoch': 0.01}
 20%|██        | 20/100 [02:59<11:01,  8.26s/it][INFO|trainer.py:3081] 2023-08-14 16:27:02,426 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:27:02,427 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:27:02,427 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 6.431786060333252, 'eval_runtime': 2.1592, 'eval_samples_per_second': 46.312, 'eval_steps_per_second': 0.463, 'epoch': 0.01}
 20%|██        | 20/100 [03:01<11:01,  8.26s/it]
100%|██████████| 1/1 [00:00<00:00,  2.28it/s][A
                                             [ASave Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:27:14,351 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:27:14,358 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20
 21%|██        | 21/100 [03:22<16:56, 12.87s/it][2023-08-14 16:27:33,705] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16
 22%|██▏       | 22/100 [03:30<14:42, 11.32s/it] 23%|██▎       | 23/100 [03:39<13:26, 10.47s/it] 24%|██▍       | 24/100 [03:47<12:21,  9.75s/it] 25%|██▌       | 25/100 [03:55<11:36,  9.28s/it] 26%|██▌       | 26/100 [04:03<10:59,  8.92s/it] 27%|██▋       | 27/100 [04:11<10:32,  8.67s/it] 28%|██▊       | 28/100 [04:19<10:11,  8.50s/it][2023-08-14 16:28:30,412] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8
 29%|██▉       | 29/100 [04:27<09:46,  8.26s/it] 30%|███       | 30/100 [04:35<09:34,  8.20s/it]                                                {'loss': 6.0448, 'learning_rate': 0.00018888354486549237, 'epoch': 0.01}
 30%|███       | 30/100 [04:35<09:34,  8.20s/it][INFO|trainer.py:3081] 2023-08-14 16:28:38,517 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:28:38,519 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:28:38,519 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 5.804974555969238, 'eval_runtime': 2.1289, 'eval_samples_per_second': 46.972, 'eval_steps_per_second': 0.47, 'epoch': 0.01}
 30%|███       | 30/100 [04:37<09:34,  8.20s/it]
100%|██████████| 1/1 [00:00<00:00,  1.14it/s][A
                                             [ASave Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:28:50,674 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:28:50,690 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30
[2023-08-14 16:29:02,206] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4
 31%|███       | 31/100 [04:59<14:47, 12.86s/it] 32%|███▏      | 32/100 [05:07<12:57, 11.44s/it] 33%|███▎      | 33/100 [05:15<11:39, 10.45s/it] 34%|███▍      | 34/100 [05:23<10:42,  9.74s/it][2023-08-14 16:29:34,270] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2
 35%|███▌      | 35/100 [05:31<09:53,  9.13s/it] 36%|███▌      | 36/100 [05:39<09:26,  8.84s/it] 37%|███▋      | 37/100 [05:47<09:02,  8.61s/it] 38%|███▊      | 38/100 [05:55<08:44,  8.46s/it][2023-08-14 16:30:06,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1
 39%|███▉      | 39/100 [06:03<08:22,  8.23s/it] 40%|████      | 40/100 [06:11<08:10,  8.18s/it]                                                {'loss': 5.5789, 'learning_rate': 0.0001766044443118978, 'epoch': 0.01}
 40%|████      | 40/100 [06:11<08:10,  8.18s/it][INFO|trainer.py:3081] 2023-08-14 16:30:14,380 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:30:14,381 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:30:14,381 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 5.527657985687256, 'eval_runtime': 2.1301, 'eval_samples_per_second': 46.946, 'eval_steps_per_second': 0.469, 'epoch': 0.01}
 40%|████      | 40/100 [06:13<08:10,  8.18s/it]
100%|██████████| 1/1 [00:00<00:00,  1.15it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:30:16,518 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-10] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:30:26,475 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:30:26,480 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40
 41%|████      | 41/100 [06:35<12:39, 12.88s/it][2023-08-14 16:30:45,825] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 0
 42%|████▏     | 42/100 [06:42<10:55, 11.30s/it] 43%|████▎     | 43/100 [06:50<09:49, 10.35s/it] 44%|████▍     | 44/100 [06:59<09:02,  9.68s/it] 45%|████▌     | 45/100 [07:07<08:25,  9.19s/it] 46%|████▌     | 46/100 [07:15<08:01,  8.92s/it] 47%|████▋     | 47/100 [07:23<07:39,  8.68s/it] 48%|████▊     | 48/100 [07:31<07:22,  8.50s/it] 49%|████▉     | 49/100 [07:39<07:06,  8.37s/it] 50%|█████     | 50/100 [07:47<06:53,  8.26s/it]                                                {'loss': 5.2363, 'learning_rate': 0.00015539200638661104, 'epoch': 0.01}
 50%|█████     | 50/100 [07:47<06:53,  8.26s/it][INFO|trainer.py:3081] 2023-08-14 16:31:50,701 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:31:50,716 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:31:50,716 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 5.132055759429932, 'eval_runtime': 2.1435, 'eval_samples_per_second': 46.653, 'eval_steps_per_second': 0.467, 'epoch': 0.01}
 50%|█████     | 50/100 [07:49<06:53,  8.26s/it]
100%|██████████| 1/1 [00:00<00:00,  1.17it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:31:52,850 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-20] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:32:02,801 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:32:02,808 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50
 51%|█████     | 51/100 [08:11<10:35, 12.98s/it] 52%|█████▏    | 52/100 [08:19<09:13, 11.53s/it] 53%|█████▎    | 53/100 [08:27<08:12, 10.48s/it] 54%|█████▍    | 54/100 [08:36<07:31,  9.81s/it] 55%|█████▌    | 55/100 [08:44<06:58,  9.29s/it] 56%|█████▌    | 56/100 [08:52<06:32,  8.92s/it] 57%|█████▋    | 57/100 [09:00<06:12,  8.66s/it] 58%|█████▊    | 58/100 [09:08<05:56,  8.48s/it] 59%|█████▉    | 59/100 [09:16<05:42,  8.36s/it] 60%|██████    | 60/100 [09:24<05:31,  8.28s/it]                                                {'loss': 4.9093, 'learning_rate': 0.00012664738136900348, 'epoch': 0.02}
 60%|██████    | 60/100 [09:24<05:31,  8.28s/it][INFO|trainer.py:3081] 2023-08-14 16:33:27,532 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:33:27,533 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:33:27,533 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 4.753652572631836, 'eval_runtime': 2.1297, 'eval_samples_per_second': 46.954, 'eval_steps_per_second': 0.47, 'epoch': 0.02}
 60%|██████    | 60/100 [09:26<05:31,  8.28s/it]
100%|██████████| 1/1 [00:00<00:00,  1.12it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:33:29,668 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-30] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:33:39,594 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:33:39,600 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60
 61%|██████    | 61/100 [09:48<08:30, 13.09s/it] 62%|██████▏   | 62/100 [09:56<07:20, 11.59s/it] 63%|██████▎   | 63/100 [10:05<06:30, 10.57s/it] 64%|██████▍   | 64/100 [10:13<05:54,  9.83s/it] 65%|██████▌   | 65/100 [10:21<05:24,  9.28s/it] 66%|██████▌   | 66/100 [10:29<05:02,  8.90s/it] 67%|██████▋   | 67/100 [10:37<04:44,  8.63s/it] 68%|██████▊   | 68/100 [10:45<04:31,  8.48s/it] 69%|██████▉   | 69/100 [10:53<04:19,  8.38s/it] 70%|███████   | 70/100 [11:01<04:08,  8.29s/it]                                                {'loss': 4.6243, 'learning_rate': 9.524180841762577e-05, 'epoch': 0.02}
 70%|███████   | 70/100 [11:01<04:08,  8.29s/it][INFO|trainer.py:3081] 2023-08-14 16:35:04,623 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:35:04,623 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:35:04,623 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 4.464716911315918, 'eval_runtime': 2.1262, 'eval_samples_per_second': 47.032, 'eval_steps_per_second': 0.47, 'epoch': 0.02}
 70%|███████   | 70/100 [11:03<04:08,  8.29s/it]
100%|██████████| 1/1 [00:00<00:00,  1.15it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:35:06,760 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-40] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:35:16,753 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:35:16,759 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70
 71%|███████   | 71/100 [11:26<06:21, 13.17s/it] 72%|███████▏  | 72/100 [11:34<05:26, 11.64s/it] 73%|███████▎  | 73/100 [11:42<04:45, 10.56s/it] 74%|███████▍  | 74/100 [11:50<04:15,  9.81s/it] 75%|███████▌  | 75/100 [11:58<03:52,  9.29s/it] 76%|███████▌  | 76/100 [12:06<03:34,  8.93s/it] 77%|███████▋  | 77/100 [12:14<03:19,  8.67s/it] 78%|███████▊  | 78/100 [12:22<03:06,  8.49s/it] 79%|███████▉  | 79/100 [12:30<02:55,  8.36s/it] 80%|████████  | 80/100 [12:38<02:45,  8.27s/it]                                                {'loss': 4.4105, 'learning_rate': 6.431137784081282e-05, 'epoch': 0.02}
 80%|████████  | 80/100 [12:38<02:45,  8.27s/it][INFO|trainer.py:3081] 2023-08-14 16:36:41,769 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:36:41,770 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:36:41,770 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 4.235877990722656, 'eval_runtime': 2.1236, 'eval_samples_per_second': 47.089, 'eval_steps_per_second': 0.471, 'epoch': 0.02}
 80%|████████  | 80/100 [12:40<02:45,  8.27s/it]
100%|██████████| 1/1 [00:00<00:00,  1.16it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:36:43,898 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-50] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-80
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:36:53,783 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:36:53,787 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-80/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-80
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-80
 81%|████████  | 81/100 [13:02<04:04, 12.86s/it] 82%|████████▏ | 82/100 [13:10<03:25, 11.43s/it] 83%|████████▎ | 83/100 [13:18<02:57, 10.42s/it] 84%|████████▍ | 84/100 [13:26<02:35,  9.71s/it] 85%|████████▌ | 85/100 [13:34<02:18,  9.22s/it] 86%|████████▌ | 86/100 [13:42<02:04,  8.88s/it] 87%|████████▋ | 87/100 [13:50<01:52,  8.62s/it] 88%|████████▊ | 88/100 [13:58<01:41,  8.45s/it] 89%|████████▉ | 89/100 [14:06<01:31,  8.32s/it] 90%|█████████ | 90/100 [14:14<01:22,  8.24s/it]                                                {'loss': 4.1817, 'learning_rate': 3.694473329154778e-05, 'epoch': 0.02}
 90%|█████████ | 90/100 [14:14<01:22,  8.24s/it][INFO|trainer.py:3081] 2023-08-14 16:38:17,836 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:38:17,836 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:38:17,836 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A{'eval_loss': 4.086572170257568, 'eval_runtime': 2.1268, 'eval_samples_per_second': 47.019, 'eval_steps_per_second': 0.47, 'epoch': 0.02}
 90%|█████████ | 90/100 [14:16<01:22,  8.24s/it]
100%|██████████| 1/1 [00:00<00:00,  1.22it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:38:19,967 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-60] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-90
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:38:29,806 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-90/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:38:29,810 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-90/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-90
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-90
 91%|█████████ | 91/100 [14:38<01:55, 12.78s/it] 92%|█████████▏| 92/100 [14:46<01:31, 11.46s/it] 93%|█████████▎| 93/100 [14:54<01:13, 10.46s/it] 94%|█████████▍| 94/100 [15:02<00:58,  9.75s/it] 95%|█████████▌| 95/100 [15:10<00:46,  9.28s/it] 96%|█████████▌| 96/100 [15:19<00:35,  8.91s/it] 97%|█████████▋| 97/100 [15:27<00:25,  8.64s/it] 98%|█████████▊| 98/100 [15:35<00:16,  8.48s/it] 99%|█████████▉| 99/100 [15:43<00:08,  8.35s/it]100%|██████████| 100/100 [15:51<00:00,  8.29s/it]                                                 {'loss': 4.0542, 'learning_rate': 1.587464671688187e-05, 'epoch': 0.03}
100%|██████████| 100/100 [15:51<00:00,  8.29s/it][INFO|trainer.py:3081] 2023-08-14 16:39:54,359 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:39:54,360 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:39:54,360 >>   Batch size = 16

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A{'eval_loss': 3.9895834922790527, 'eval_runtime': 2.1368, 'eval_samples_per_second': 46.799, 'eval_steps_per_second': 0.468, 'epoch': 0.03}
100%|██████████| 100/100 [15:53<00:00,  8.29s/it]
100%|██████████| 1/1 [00:00<00:00,  1.18it/s][A
                                             [A[INFO|trainer.py:2894] 2023-08-14 16:39:56,500 >> Deleting older checkpoint [20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-70] due to args.save_total_limit
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-100
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:40:06,341 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:40:06,347 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-100/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-100
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/checkpoint-100
[INFO|trainer.py:1934] 2023-08-14 16:40:10,094 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 967.0773, 'train_samples_per_second': 13.236, 'train_steps_per_second': 0.103, 'train_loss': 5.483873615264892, 'epoch': 0.03}
100%|██████████| 100/100 [16:07<00:00,  8.29s/it]100%|██████████| 100/100 [16:07<00:00,  9.67s/it]
***** train metrics *****
  epoch                    =       0.03
  train_loss               =     5.4839
  train_runtime            = 0:16:07.07
  train_samples            =     500000
  train_samples_per_second =     13.236
  train_steps_per_second   =      0.103
Save Peft Config at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/last_model
[INFO|tokenization_utils_base.py:2210] 2023-08-14 16:40:22,532 >> tokenizer config file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/last_model/tokenizer_config.json
[INFO|tokenization_utils_base.py:2217] 2023-08-14 16:40:22,537 >> Special tokens file saved in 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/last_model/special_tokens_map.json
Save Tokenizer at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/last_model
Save adapter model at 20230810_Llama-2-7B_Belle-alpaca-50W_newENV-2/last_model
08/14/2023 16:40:27 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3081] 2023-08-14 16:40:27,173 >> ***** Running Evaluation *****
[INFO|trainer.py:3083] 2023-08-14 16:40:27,174 >>   Num examples = 100
[INFO|trainer.py:3086] 2023-08-14 16:40:27,174 >>   Batch size = 16
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.18it/s]
***** eval metrics *****
  epoch                   =       0.03
  eval_loss               =     3.9896
  eval_runtime            = 0:00:02.12
  eval_samples            =        100
  eval_samples_per_second =     47.118
  eval_steps_per_second   =      0.471
  perplexity              =    54.0324
